{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b13889-404e-488c-bd11-3688b0144ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96112376/96112376 [==============================] - 4s 0us/step\n",
      "InceptionV3 chargé avec succès.\n",
      "Format de sortie des features : (None, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# 1. Charger InceptionV3\n",
    "# weights='imagenet' : On utilise le savoir pré-acquis\n",
    "# include_top=False  : On jette la dernière couche de classification (celle qui dit \"chat\", \"chien\")\n",
    "# pooling='avg'      : Important ! On transforme la sortie (8x8x2048) en un vecteur plat (2048)\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# On crée un nouveau modèle qui s'arrête juste avant la fin\n",
    "# La couche finale d'InceptionV3 s'appelle souvent 'avg_pool' ou on prend l'avant dernière\n",
    "model_inception = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "print(\"InceptionV3 chargé avec succès.\")\n",
    "print(\"Format de sortie des features :\", model_inception.output_shape)\n",
    "# Doit afficher (None, 2048) -> Un vecteur de 2048 nombres par image\n",
    "\n",
    "# 2. Fonction de prétraitement d'une image\n",
    "def load_and_process_image(image_path):\n",
    "    # InceptionV3 attend obligatoirement du 299x299\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    \n",
    "    # Conversion en tableau numpy\n",
    "    x = img_to_array(img)\n",
    "    \n",
    "    # Ajout d'une dimension (pour faire un batch de 1 image) : (299, 299, 3) -> (1, 299, 299, 3)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    # Prétraitement spécifique à Inception (Mise à l'échelle entre -1 et 1)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5abf42-d4f9-4d03-a4ee-434e4c1c7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier JSON...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChargement du fichier JSON...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mANNOTATION_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Mapping Image -> Caption\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\A5 ia conda\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ANNOTATION_FILE = '/content/annotations/captions_train2014.json'\n",
    "IMAGE_FOLDER = '/content/train2014/'\n",
    "NUM_EXAMPLES = 10000  # On garde 10 000 images pour le prototype\n",
    "\n",
    "# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\n",
    "print(\"Chargement du fichier JSON...\")\n",
    "with open(ANNOTATION_FILE, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Mapping Image -> Caption\n",
    "image_path_to_caption = {}\n",
    "for annot in annotations['annotations']:\n",
    "    caption = f\"<start> {annot['caption']} <end>\"\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = os.path.join(IMAGE_FOLDER, 'COCO_train2014_' + '%012d.jpg' % (image_id))\n",
    "\n",
    "    if full_coco_image_path in image_path_to_caption:\n",
    "        image_path_to_caption[full_coco_image_path].append(caption)\n",
    "    else:\n",
    "        image_path_to_caption[full_coco_image_path] = [caption]\n",
    "\n",
    "# Sélection aléatoire des images\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "import random\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:NUM_EXAMPLES]\n",
    "\n",
    "print(f\"Nombre d'images sélectionnées : {len(train_image_paths)}\")\n",
    "\n",
    "# --- 2. PRÉPARATION DU MODÈLE ---\n",
    "print(\"Chargement d'InceptionV3...\")\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "# On prend la sortie avant la classification finale\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "# --- 3. EXTRACTION DES FEATURES ---\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création du dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction... (Patientez)\")\n",
    "\n",
    "batch_count = 0\n",
    "for img, path in image_dataset:\n",
    "    # Extraction\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # Reshape (batch, 8, 8, 2048) ou (batch, 2048) -> On veut (batch, 2048)\n",
    "    # Souvent Inception sort (batch, 2048) directement avec layers[-2]\n",
    "    # Mais par sécurité on s'assure que c'est compatible\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # Sauvegarde .npy\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d929a4b8-7d46-4abf-b25d-74fae65721e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, image_path\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Création d'un dataset TensorFlow pour charger les images efficacement\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\u001b[43mtrain_image_paths\u001b[49m)\n\u001b[0;32m     11\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m image_dataset\u001b[38;5;241m.\u001b[39mmap(load_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDémarrage de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextraction des features... (Cela peut prendre du temps)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Fonction de chargement et preprocessing d'une image\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création d'un dataset TensorFlow pour charger les images efficacement\n",
    "# Note : on suppose que 'train_image_paths' et 'feature_extractor' sont déjà définis par l'étape précédente\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction des features... (Cela peut prendre du temps)\")\n",
    "\n",
    "# Compteur pour suivre la progression manuellement\n",
    "batch_count = 0\n",
    "\n",
    "# Boucle d'extraction (Sans tqdm)\n",
    "for img, path in image_dataset:\n",
    "    # 1. Extraction des features via InceptionV3\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # 2. Reshape : On s'assure que c'est bien (batch_size, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # 3. Sauvegarde sur le disque (.npy) pour chaque image\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # On enregistre le fichier au même endroit que l'image mais avec extension .npy\n",
    "        np.save(path_of_feature + '.npy', bf.numpy())\n",
    "\n",
    "    # Affichage de progression tous les 100 batches\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        print(f\"Batch n°{batch_count} traité...\")\n",
    "\n",
    "print(\"Extraction terminée ! Toutes les images sont converties en vecteurs .npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f233-45dd-4536-85bc-664abcc856bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
