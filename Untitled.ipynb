{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b13889-404e-488c-bd11-3688b0144ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96112376/96112376 [==============================] - 4s 0us/step\n",
      "InceptionV3 chargé avec succès.\n",
      "Format de sortie des features : (None, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# 1. Charger InceptionV3\n",
    "# weights='imagenet' : On utilise le savoir pré-acquis\n",
    "# include_top=False  : On jette la dernière couche de classification (celle qui dit \"chat\", \"chien\")\n",
    "# pooling='avg'      : Important ! On transforme la sortie (8x8x2048) en un vecteur plat (2048)\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# On crée un nouveau modèle qui s'arrête juste avant la fin\n",
    "# La couche finale d'InceptionV3 s'appelle souvent 'avg_pool' ou on prend l'avant dernière\n",
    "model_inception = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "print(\"InceptionV3 chargé avec succès.\")\n",
    "print(\"Format de sortie des features :\", model_inception.output_shape)\n",
    "# Doit afficher (None, 2048) -> Un vecteur de 2048 nombres par image\n",
    "\n",
    "# 2. Fonction de prétraitement d'une image\n",
    "def load_and_process_image(image_path):\n",
    "    # InceptionV3 attend obligatoirement du 299x299\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    \n",
    "    # Conversion en tableau numpy\n",
    "    x = img_to_array(img)\n",
    "    \n",
    "    # Ajout d'une dimension (pour faire un batch de 1 image) : (299, 299, 3) -> (1, 299, 299, 3)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    # Prétraitement spécifique à Inception (Mise à l'échelle entre -1 et 1)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5abf42-d4f9-4d03-a4ee-434e4c1c7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier JSON...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChargement du fichier JSON...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mANNOTATION_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Mapping Image -> Caption\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\A5 ia conda\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ANNOTATION_FILE = '/content/annotations/captions_train2014.json'\n",
    "IMAGE_FOLDER = '/content/train2014/'\n",
    "NUM_EXAMPLES = 10000  # On garde 10 000 images pour le prototype\n",
    "\n",
    "# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\n",
    "print(\"Chargement du fichier JSON...\")\n",
    "with open(ANNOTATION_FILE, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Mapping Image -> Caption\n",
    "image_path_to_caption = {}\n",
    "for annot in annotations['annotations']:\n",
    "    caption = f\"<start> {annot['caption']} <end>\"\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = os.path.join(IMAGE_FOLDER, 'COCO_train2014_' + '%012d.jpg' % (image_id))\n",
    "\n",
    "    if full_coco_image_path in image_path_to_caption:\n",
    "        image_path_to_caption[full_coco_image_path].append(caption)\n",
    "    else:\n",
    "        image_path_to_caption[full_coco_image_path] = [caption]\n",
    "\n",
    "# Sélection aléatoire des images\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "import random\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:NUM_EXAMPLES]\n",
    "\n",
    "print(f\"Nombre d'images sélectionnées : {len(train_image_paths)}\")\n",
    "\n",
    "# --- 2. PRÉPARATION DU MODÈLE ---\n",
    "print(\"Chargement d'InceptionV3...\")\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "# On prend la sortie avant la classification finale\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "# --- 3. EXTRACTION DES FEATURES ---\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création du dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction... (Patientez)\")\n",
    "\n",
    "batch_count = 0\n",
    "for img, path in image_dataset:\n",
    "    # Extraction\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # Reshape (batch, 8, 8, 2048) ou (batch, 2048) -> On veut (batch, 2048)\n",
    "    # Souvent Inception sort (batch, 2048) directement avec layers[-2]\n",
    "    # Mais par sécurité on s'assure que c'est compatible\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # Sauvegarde .npy\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d929a4b8-7d46-4abf-b25d-74fae65721e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, image_path\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Création d'un dataset TensorFlow pour charger les images efficacement\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\u001b[43mtrain_image_paths\u001b[49m)\n\u001b[0;32m     11\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m image_dataset\u001b[38;5;241m.\u001b[39mmap(load_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDémarrage de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextraction des features... (Cela peut prendre du temps)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Fonction de chargement et preprocessing d'une image\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création d'un dataset TensorFlow pour charger les images efficacement\n",
    "# Note : on suppose que 'train_image_paths' et 'feature_extractor' sont déjà définis par l'étape précédente\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction des features... (Cela peut prendre du temps)\")\n",
    "\n",
    "# Compteur pour suivre la progression manuellement\n",
    "batch_count = 0\n",
    "\n",
    "# Boucle d'extraction (Sans tqdm)\n",
    "for img, path in image_dataset:\n",
    "    # 1. Extraction des features via InceptionV3\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # 2. Reshape : On s'assure que c'est bien (batch_size, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # 3. Sauvegarde sur le disque (.npy) pour chaque image\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # On enregistre le fichier au même endroit que l'image mais avec extension .npy\n",
    "        np.save(path_of_feature + '.npy', bf.numpy())\n",
    "\n",
    "    # Affichage de progression tous les 100 batches\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        print(f\"Batch n°{batch_count} traité...\")\n",
    "\n",
    "print(\"Extraction terminée ! Toutes les images sont converties en vecteurs .npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f233-45dd-4536-85bc-664abcc856bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. CONFIGURATION DU TEXTE ---\n",
    "# On prend les 5000 mots les plus courants (suffisant pour un prototype)\n",
    "TOP_K = 5000 \n",
    "\n",
    "# On récupère toutes les légendes dans une liste unique\n",
    "all_captions = []\n",
    "for key in train_image_paths:\n",
    "    for caption in image_path_to_caption[key]:\n",
    "        all_captions.append(caption)\n",
    "\n",
    "# --- 2. TOKENIZATION (Apprendre le vocabulaire) ---\n",
    "print(\"Apprentissage du vocabulaire...\")\n",
    "tokenizer = Tokenizer(num_words=TOP_K, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# On ajoute un token spécial pour le \"padding\" (remplissage) s'il n'existe pas\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Création des séquences (Texte -> Chiffres)\n",
    "train_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Calcul de la taille maximale d'une phrase (pour que toutes aient la même taille)\n",
    "max_length = max(len(seq) for seq in train_seqs)\n",
    "print(f\"Vocabulaire : {TOP_K} mots\")\n",
    "print(f\"Phrase la plus longue : {max_length} mots\")\n",
    "\n",
    "# On stocke les vecteurs d'images et les captions correspondantes\n",
    "# Pour simplifier, on associe chaque fichier .npy à ses captions\n",
    "img_name_vector = []\n",
    "train_captions_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    for c in caption_list:\n",
    "        # On ajoute le chemin vers le fichier .npy (pas l'image jpg !)\n",
    "        img_name_vector.append(image_path + '.npy') \n",
    "        train_captions_vector.append(c)\n",
    "\n",
    "# Transformation finale des textes en vecteurs de même longueur\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions_vector)\n",
    "cap_vector = pad_sequences(train_seqs, maxlen=max_length, padding='post')\n",
    "\n",
    "# --- 3. SÉPARATION TRAIN / VALIDATION (80% - 20%) ---\n",
    "img_train, img_val, cap_train, cap_val = train_test_split(img_name_vector, cap_vector, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Données d'entraînement : {len(img_train)} exemples\")\n",
    "print(f\"Données de validation : {len(img_val)} exemples\")\n",
    "\n",
    "# --- 4. CRÉATION DU DATASET TF.DATA (Optimisé) ---\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = TOP_K + 1\n",
    "\n",
    "# Fonction qui charge le vecteur .npy et renvoie (Image, Caption)\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8'))\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_train, cap_train))\n",
    "\n",
    "# On utilise map avec numpy_function car np.load ne marche pas directement en pur TensorFlow\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# Pareil pour la validation\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((img_val, cap_val))\n",
    "dataset_val = dataset_val.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Dataset prêt à l'emploi !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
