{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3b13889-404e-488c-bd11-3688b0144ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96112376/96112376 [==============================] - 4s 0us/step\n",
      "InceptionV3 chargé avec succès.\n",
      "Format de sortie des features : (None, 2048)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# 1. Charger InceptionV3\n",
    "# weights='imagenet' : On utilise le savoir pré-acquis\n",
    "# include_top=False  : On jette la dernière couche de classification (celle qui dit \"chat\", \"chien\")\n",
    "# pooling='avg'      : Important ! On transforme la sortie (8x8x2048) en un vecteur plat (2048)\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "\n",
    "# On crée un nouveau modèle qui s'arrête juste avant la fin\n",
    "# La couche finale d'InceptionV3 s'appelle souvent 'avg_pool' ou on prend l'avant dernière\n",
    "model_inception = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "print(\"InceptionV3 chargé avec succès.\")\n",
    "print(\"Format de sortie des features :\", model_inception.output_shape)\n",
    "# Doit afficher (None, 2048) -> Un vecteur de 2048 nombres par image\n",
    "\n",
    "# 2. Fonction de prétraitement d'une image\n",
    "def load_and_process_image(image_path):\n",
    "    # InceptionV3 attend obligatoirement du 299x299\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    \n",
    "    # Conversion en tableau numpy\n",
    "    x = img_to_array(img)\n",
    "    \n",
    "    # Ajout d'une dimension (pour faire un batch de 1 image) : (299, 299, 3) -> (1, 299, 299, 3)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    # Prétraitement spécifique à Inception (Mise à l'échelle entre -1 et 1)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5abf42-d4f9-4d03-a4ee-434e4c1c7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du fichier JSON...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChargement du fichier JSON...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mANNOTATION_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     18\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Mapping Image -> Caption\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\A5 ia conda\\env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/captions_train2014.json'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ANNOTATION_FILE = '/content/annotations/captions_train2014.json'\n",
    "IMAGE_FOLDER = '/content/train2014/'\n",
    "NUM_EXAMPLES = 10000  # On garde 10 000 images pour le prototype\n",
    "\n",
    "# --- 1. CHARGEMENT DES LÉGENDES (JSON) ---\n",
    "print(\"Chargement du fichier JSON...\")\n",
    "with open(ANNOTATION_FILE, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Mapping Image -> Caption\n",
    "image_path_to_caption = {}\n",
    "for annot in annotations['annotations']:\n",
    "    caption = f\"<start> {annot['caption']} <end>\"\n",
    "    image_id = annot['image_id']\n",
    "    full_coco_image_path = os.path.join(IMAGE_FOLDER, 'COCO_train2014_' + '%012d.jpg' % (image_id))\n",
    "\n",
    "    if full_coco_image_path in image_path_to_caption:\n",
    "        image_path_to_caption[full_coco_image_path].append(caption)\n",
    "    else:\n",
    "        image_path_to_caption[full_coco_image_path] = [caption]\n",
    "\n",
    "# Sélection aléatoire des images\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "import random\n",
    "random.shuffle(image_paths)\n",
    "train_image_paths = image_paths[:NUM_EXAMPLES]\n",
    "\n",
    "print(f\"Nombre d'images sélectionnées : {len(train_image_paths)}\")\n",
    "\n",
    "# --- 2. PRÉPARATION DU MODÈLE ---\n",
    "print(\"Chargement d'InceptionV3...\")\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "# On prend la sortie avant la classification finale\n",
    "feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "# --- 3. EXTRACTION DES FEATURES ---\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création du dataset\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction... (Patientez)\")\n",
    "\n",
    "batch_count = 0\n",
    "for img, path in image_dataset:\n",
    "    # Extraction\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # Reshape (batch, 8, 8, 2048) ou (batch, 2048) -> On veut (batch, 2048)\n",
    "    # Souvent Inception sort (batch, 2048) directement avec layers[-2]\n",
    "    # Mais par sécurité on s'assure que c'est compatible\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # Sauvegarde .npy\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d929a4b8-7d46-4abf-b25d-74fae65721e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_image_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, image_path\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Création d'un dataset TensorFlow pour charger les images efficacement\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(\u001b[43mtrain_image_paths\u001b[49m)\n\u001b[0;32m     11\u001b[0m image_dataset \u001b[38;5;241m=\u001b[39m image_dataset\u001b[38;5;241m.\u001b[39mmap(load_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDémarrage de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mextraction des features... (Cela peut prendre du temps)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_image_paths' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Fonction de chargement et preprocessing d'une image\n",
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "# Création d'un dataset TensorFlow pour charger les images efficacement\n",
    "# Note : on suppose que 'train_image_paths' et 'feature_extractor' sont déjà définis par l'étape précédente\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(train_image_paths)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n",
    "\n",
    "print(\"Démarrage de l'extraction des features... (Cela peut prendre du temps)\")\n",
    "\n",
    "# Compteur pour suivre la progression manuellement\n",
    "batch_count = 0\n",
    "\n",
    "# Boucle d'extraction (Sans tqdm)\n",
    "for img, path in image_dataset:\n",
    "    # 1. Extraction des features via InceptionV3\n",
    "    batch_features = feature_extractor(img)\n",
    "    \n",
    "    # 2. Reshape : On s'assure que c'est bien (batch_size, 2048)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, 2048))\n",
    "\n",
    "    # 3. Sauvegarde sur le disque (.npy) pour chaque image\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        # On enregistre le fichier au même endroit que l'image mais avec extension .npy\n",
    "        np.save(path_of_feature + '.npy', bf.numpy())\n",
    "\n",
    "    # Affichage de progression tous les 100 batches\n",
    "    batch_count += 1\n",
    "    if batch_count % 100 == 0:\n",
    "        print(f\"Batch n°{batch_count} traité...\")\n",
    "\n",
    "print(\"Extraction terminée ! Toutes les images sont converties en vecteurs .npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5f233-45dd-4536-85bc-664abcc856bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. CONFIGURATION DU TEXTE ---\n",
    "# On prend les 5000 mots les plus courants (suffisant pour un prototype)\n",
    "TOP_K = 5000 \n",
    "\n",
    "# On récupère toutes les légendes dans une liste unique\n",
    "all_captions = []\n",
    "for key in train_image_paths:\n",
    "    for caption in image_path_to_caption[key]:\n",
    "        all_captions.append(caption)\n",
    "\n",
    "# --- 2. TOKENIZATION (Apprendre le vocabulaire) ---\n",
    "print(\"Apprentissage du vocabulaire...\")\n",
    "tokenizer = Tokenizer(num_words=TOP_K, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# On ajoute un token spécial pour le \"padding\" (remplissage) s'il n'existe pas\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "\n",
    "# Création des séquences (Texte -> Chiffres)\n",
    "train_seqs = tokenizer.texts_to_sequences(all_captions)\n",
    "\n",
    "# Calcul de la taille maximale d'une phrase (pour que toutes aient la même taille)\n",
    "max_length = max(len(seq) for seq in train_seqs)\n",
    "print(f\"Vocabulaire : {TOP_K} mots\")\n",
    "print(f\"Phrase la plus longue : {max_length} mots\")\n",
    "\n",
    "# On stocke les vecteurs d'images et les captions correspondantes\n",
    "# Pour simplifier, on associe chaque fichier .npy à ses captions\n",
    "img_name_vector = []\n",
    "train_captions_vector = []\n",
    "\n",
    "for image_path in train_image_paths:\n",
    "    caption_list = image_path_to_caption[image_path]\n",
    "    for c in caption_list:\n",
    "        # On ajoute le chemin vers le fichier .npy (pas l'image jpg !)\n",
    "        img_name_vector.append(image_path + '.npy') \n",
    "        train_captions_vector.append(c)\n",
    "\n",
    "# Transformation finale des textes en vecteurs de même longueur\n",
    "train_seqs = tokenizer.texts_to_sequences(train_captions_vector)\n",
    "cap_vector = pad_sequences(train_seqs, maxlen=max_length, padding='post')\n",
    "\n",
    "# --- 3. SÉPARATION TRAIN / VALIDATION (80% - 20%) ---\n",
    "img_train, img_val, cap_train, cap_val = train_test_split(img_name_vector, cap_vector, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Données d'entraînement : {len(img_train)} exemples\")\n",
    "print(f\"Données de validation : {len(img_val)} exemples\")\n",
    "\n",
    "# --- 4. CRÉATION DU DATASET TF.DATA (Optimisé) ---\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = TOP_K + 1\n",
    "\n",
    "# Fonction qui charge le vecteur .npy et renvoie (Image, Caption)\n",
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8'))\n",
    "    return img_tensor, cap\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((img_train, cap_train))\n",
    "\n",
    "# On utilise map avec numpy_function car np.load ne marche pas directement en pur TensorFlow\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# Pareil pour la validation\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((img_val, cap_val))\n",
    "dataset_val = dataset_val.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Dataset prêt à l'emploi !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e28d93-ff24-4c41-b487-84cdab7b9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DÉFINITION DU MODÈLE (Encoder-Decoder avec Attention) ---\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features shape: (batch_size, 64, embedding_dim)\n",
    "    # hidden shape: (batch_size, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "    \n",
    "    # Calcul du score d'attention\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "    score = self.V(attention_hidden_layer)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # L'encodeur a déjà été fait par InceptionV3, ici on ajoute juste une couche Dense pour adapter la dimension\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # x = mot précédent\n",
    "    # features = image\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    x = self.fc1(output)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "    x = self.fc2(x)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n",
    "\n",
    "# --- INSTANCIATION ---\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "# --- BOUCLE D'ENTRAÎNEMENT ---\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "  hidden = decoder.reset_state(target.shape[0])\n",
    "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "      \n",
    "      # On prédit mot par mot\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passage dans le décodeur\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "          # Teacher forcing : le mot suivant est le mot correct\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "  return total_loss\n",
    "\n",
    "print(\"Modèle construit. Démarrage de l'entraînement...\")\n",
    "\n",
    "# --- LANCEMENT ---\n",
    "EPOCHS = 10 # 10 ou 20 époques suffisent pour voir des résultats\n",
    "import time\n",
    "\n",
    "loss_plot = []\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Itération sur les batches (sans tqdm pour éviter les erreurs)\n",
    "    batch = 0\n",
    "    for (img_tensor, target) in dataset:\n",
    "        batch_loss = train_step(img_tensor, target)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "        batch += 1\n",
    "\n",
    "    loss_plot.append(total_loss / batch)\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/batch:.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
    "\n",
    "print(\"Entraînement terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a35de-59ea-4460-932d-162e0b538fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate(image_path):\n",
    "    attention_plot = np.zeros((max_length, 2048)) # Placeholder pour l'attention\n",
    "\n",
    "    # 1. Prétraitement de l'image (comme pour l'entraînement)\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    \n",
    "    # 2. Extraction des features avec InceptionV3\n",
    "    # On ajoute la dimension batch : (299, 299, 3) -> (1, 299, 299, 3)\n",
    "    features = feature_extractor(tf.expand_dims(img, 0))\n",
    "    \n",
    "    # Reshape pour correspondre à ce qu'attend l'encodeur\n",
    "    features = tf.reshape(features, (features.shape[0], -1, 2048))\n",
    "\n",
    "    # 3. Passage dans l'Encodeur entraîné\n",
    "    features = encoder(features)\n",
    "\n",
    "    # 4. Initialisation du Décodeur\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    # 5. Boucle de génération (mot par mot)\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        # On prend l'index du mot avec la plus haute probabilité\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        \n",
    "        # On retrouve le mot texte grâce au tokenizer\n",
    "        if predicted_id in tokenizer.index_word:\n",
    "            result.append(tokenizer.index_word[predicted_id])\n",
    "        else:\n",
    "            result.append('<unk>') # Mot inconnu\n",
    "\n",
    "        # Si le modèle prédit la fin de phrase, on arrête\n",
    "        if tokenizer.index_word.get(predicted_id) == '<end>':\n",
    "            return result\n",
    "\n",
    "        # Le mot prédit devient l'entrée pour le tour suivant\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "# --- FONCTION D'AFFICHAGE ---\n",
    "def generate_caption(image_path):\n",
    "    # Générer la légende\n",
    "    result = evaluate(image_path)\n",
    "    \n",
    "    # Afficher l'image\n",
    "    image = plt.imread(image_path)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Afficher le texte généré (on enlève le <end>)\n",
    "    caption_text = ' '.join(result).replace('<end>', '')\n",
    "    plt.title(f\"Prédiction IA : {caption_text}\", fontsize=15, color='blue')\n",
    "    plt.show()\n",
    "\n",
    "# --- TEST SUR DES IMAGES DE VALIDATION ---\n",
    "# On prend 3 images au hasard dans le set de validation\n",
    "import random\n",
    "print(\"Test sur des images du dataset...\")\n",
    "for i in range(3):\n",
    "    # On récupère un chemin d'image original (.npy -> .jpg)\n",
    "    npy_path = random.choice(img_val)\n",
    "    jpg_path = npy_path.replace('.npy', '') # On retrouve le jpg original\n",
    "    \n",
    "    generate_caption(jpg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
